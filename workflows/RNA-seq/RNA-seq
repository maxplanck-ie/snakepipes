#!/usr/bin/env python3

__version__ = "0.5"

__description__ = """
RNA-seq workflow v{version} - MPI-IE workflow for RNA mapping and analysis
Fabian Kilpert, Steffen Heyne
March 13, 2017

usage example:
    RNA-seq -i input-dir -o output-dir mm10
""".format(version=__version__)


import argparse
import os
import signal
import subprocess
import sys
import textwrap
import time
import shutil
import yaml


sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))))+"/shared/")
import common_functions as cf

def parse_args(defaults):
    """
    Parse arguments from the command line.
    """

    parser = argparse.ArgumentParser(
        prog=os.path.basename(sys.argv[0]),
        formatter_class=argparse.RawDescriptionHelpFormatter, description=textwrap.dedent(__description__)
    )

    ## positional/required
    parser.add_argument("genome", metavar="GENOME", help="genome acronym of target organism (supported: 'dm6', 'hs37d5', 'mm9', 'mm10', 'SchizoSPombe_ASM294v2')")
    ## optional
    parser.add_argument("-c", "--configfile", dest="configfile", help="configuration file (default: '%(default)s')", default=defaults["configfile"])
    parser.add_argument("-i", "--input-dir", dest="indir", help="input directory containing the FASTQ files, either paired-end OR single-end data (default: '%(default)s')", default=defaults["indir"])
    parser.add_argument("-o", "--output-dir", dest="outdir", help="output directory (default: '%(default)s')", default=defaults["outdir"])
    parser.add_argument("--DE", dest="sample_info", help="Information on samples (required for DE analysis); see 'snakemake_workflows/shared/tools/sampleInfo.example.tsv' for example. IMPORTANT: The first entry defines which group of samples are control. By this, the order of comparison and likewise the sign of values can be changed! (default: '%(default)s')", default=defaults["sample_info"])
    parser.add_argument("-m", "--mode", dest="mode", help="workflow running modes (available: 'mapping-free, mapping, deepTools_qc') (default: '%(default)s')", default=defaults["mode"])
    parser.add_argument("-j", "--jobs", dest="max_jobs", metavar="INT", help="maximum number of concurrently submitted Slurm jobs / cores if workflow is run locally (default: '%(default)s')", type=int, default=defaults["max_jobs"])
    parser.add_argument("--mapping_prg", dest="mapping_prg", metavar="STR", help="Program used for mapping: STAR or HISAT2 (default: '%(default)s')", type=str, default=defaults["mapping_prg"])
    parser.add_argument("--star_options", dest="star_options", metavar="STR", help="STAR option string, e.g.: '--twopassMode Basic' (default: '%(default)s')", type=str, default=defaults["star_options"])
    parser.add_argument("--hisat_options", dest="hisat_options", metavar="STR", help="HISAT2 option string (default: '%(default)s')", type=str, default=defaults["hisat_options"])
    parser.add_argument("--local", dest="local", action="store_true", help="run workflow locally (default: '%(default)s')", default=defaults["local"])
    parser.add_argument("--snakemake_options", dest="snakemake_options", metavar="STR", type=str, help="Snakemake options to be passed directly to snakemake, e.g. use --snakemake_options='--dryrun --rerun-incomplete --unlock --forceall'. (default: '%(default)s')", default=defaults["snakemake_options"])
    parser.add_argument("--salmon_index_options", dest="salmon_index_options", metavar="STR", type=str, help="Salmon index options, e.g. '--type fmd' (default: '%(default)s')", default=defaults["salmon_index_options"])
    parser.add_argument("--featurecounts_options", dest="featurecounts_options", metavar="STR", type=str, help="featureCounts option string. The options '-p -B' are always used for paired-end data (default: '%(default)s')", default=defaults["featurecounts_options"])
    parser.add_argument("--downsample", dest="downsample", metavar="INT", help="downsample the given number of reads randomly from each FASTQ file (default: '%(default)s')", type=int, default=defaults["downsample"])
    parser.add_argument("--trim_prg", dest="trim_prg", choices=['cutadapt', 'trimgalore'], help="trimming program: Cutadapt or TrimGalore (default: '%(default)s')", default=defaults["trim_prg"])
    parser.add_argument("--trim", dest="trim", action="store_true", help="activate trimming (default: '%(default)s')", default=defaults["trim"])
    parser.add_argument("--trim_options", dest="trim_options", metavar="STR", type=str, help="Additional option string for trimming program of choice. (default: '%(default)s')", default=defaults["trim_options"])
    parser.add_argument("--fastqc", dest="fastqc", action="store_true", help="run FastQC read quality control (default: '%(default)s')", default=defaults["fastqc"])
    parser.add_argument("--library_type", dest="library_type", metavar="", help="user provided library type strand specificity. featurCounts style: 0, 1, 2 (Illumina TruSeq); default: '%(default)s')", type=int, default=defaults["library_type"])
    parser.add_argument("--filter_annotation", dest="filter_annotation", metavar="STR", type=str, help="filter annotation GTF by grep for use with Salmon, e.g. use --filter_annotation='-v pseudogene'; default: '%(default)s')", default=defaults["filter_annotation"])
    parser.add_argument("--bw-binsize", dest="bw_binsize", metavar="INT", help="bin size of output files in bigWig format (default: '%(default)s')", type=int, default=defaults["bw_binsize"])
    parser.add_argument("--tempdir", dest="tempdir", type=str, help="used prefix path for temporary directory created via mktemp. Created temp dir gets exported as $TMPDIR and is removed at the end of this wrapper! (default: '%(default)s')", default=defaults["tempdir"])
    parser.add_argument("-v", "--verbose", dest="verbose", action="store_true", help="verbose output (default: '%(default)s')", default=defaults["verbose"])

    args = parser.parse_args()

    ## Variable sanity checking
    if args.mode is str:
        args.mode = list(map( str.strip, re.split(',|;', args.mode) ))

    ## correction to variables
    try:
        args.indir = os.path.abspath(args.indir)
    except:
        args.indir = os.path.abspath(os.getcwd())

    try:
        args.outdir = os.path.abspath(args.outdir)
    except:
        args.outdir = os.path.abspath(os.getcwd())

    try:
        args.configfile = os.path.abspath(args.configfile)
    except:
        args.configfile = None

    args.cluster_logs_dir = os.path.join(args.outdir, "cluster_logs")

    if args.sample_info:
        if os.path.exists(os.path.abspath(args.sample_info)):
            args.cell_names = os.path.abspath(args.sample_info)
        else:
            print("\nSample info file not found! (--DE {})\n".format(args.sample_info))
            exit(1)
            
    return args


def main():

    ## basic paths
    this_script_dir = os.path.dirname(os.path.realpath(__file__))
    main_dir_path = os.path.join(os.path.dirname(os.path.dirname(this_script_dir)))

    ## defaults
    with open(os.path.join(this_script_dir, "defaults.yaml"), "r") as f:
        defaults = yaml.load(f)

    ## get command line arguments
    args = parse_args(defaults)

    args.this_script_dir = this_script_dir
    args.main_dir_path = main_dir_path

    ## merge configuration dicts
    config = defaults   # 1) form defaults.yaml
    if args.configfile:
        with open(args.configfile, "r") as f:
            user_config = yaml.load(f)
        config = cf.merge_dicts(config, user_config) # 2) form user_config.yaml
    config = cf.merge_dicts(config, vars(args)) # 3) from wrapper parameters

    ## Output directory + log directory
    subprocess.call("[ -d {cluster_logs_dir} ] || mkdir -p {cluster_logs_dir}".format(cluster_logs_dir=args.cluster_logs_dir), shell=True)

    ## save to configs.yaml in outdir
    with open(os.path.join(args.outdir,'config.yaml'), 'w') as f:
        yaml.dump(config, f, default_flow_style=None)

    #snakemake_module_load = "module load snakemake slurm &&".split()
    snakemake_module_load = " ".split()
    snakemake_cmd = """
                    snakemake {snakemake_options} --latency-wait 300 --snakefile {snakefile} --jobs {max_jobs} --directory {outdir} --configfile {configfile}
                    """.format( snakefile = os.path.join(args.this_script_dir, "Snakefile"),
                                max_jobs = args.max_jobs,
                                outdir = args.outdir,
                                cluster_logs_dir = os.path.abspath(args.cluster_logs_dir),
                                snakemake_options=str(args.snakemake_options or ''),
                                configfile = os.path.join(args.outdir,'config.yaml'),
                              ).split()

    if args.verbose:
        snakemake_cmd.append("--printshellcmds")

    if not args.local:
        snakemake_cmd += ["--cluster 'SlurmEasy --mem-per-cpu 12G --threads {threads} --log", args.cluster_logs_dir, "--name {rule}.snakemake'"]

    snakemake_log = "2>&1 | tee -a {}/RNA-seq.log".format(args.outdir).split()

    ## create local temp dir and add this path to environment as $TMPDIR variable
    ## on SLURM: $TMPDIR is set, created and removed by SlurmEasy on cluster node
    temp_path = cf.make_temp_dir(args.tempdir, args.outdir, args.verbose)
    snakemake_exports = ("export TMPDIR="+temp_path+" && ").split()

    cmd = " ".join(snakemake_exports + snakemake_module_load + snakemake_cmd + snakemake_log)

    if args.verbose:
        print("\n", cmd, "\n")

    ## Write snakemake_cmd to log file
    with open(os.path.join(args.outdir,"RNA-seq.log"),"w") as f:
        f.write(" ".join(sys.argv)+"\n\n")
        f.write(cmd+"\n\n")

    ## Run snakemake
    p = subprocess.Popen(cmd, shell=True)
    if args.verbose:
        print("PID:", p.pid, "\n")
    try:
        p.wait()
    except:
        print("\nWARNING: Snakemake terminated!!!")
        if p.returncode != 0:
            if p.returncode:
                print("Returncode:", p.returncode)

            # kill snakemake and child processes
            subprocess.call(["pkill", "-SIGTERM", "-P", str(p.pid)])
            print("SIGTERM sent to PID:", p.pid)

            # # kill grid engine jobs
            # time.sleep(10)
            # job_ids = subprocess.check_output("""ls {cluster_logs_dir} | awk -F "." '{{print $NF}}' | sed 's/e\|o//' | sort -u""".format(cluster_logs_dir=cluster_logs_dir), shell=True).split()
            # for job_id in job_ids:
            #     subprocess.call( "qdel {} 2>&1 >/dev/null".format(str(job_id)), shell="bash" )

    ## remove temp dir
    if (temp_path != "" and os.path.exists(temp_path)):
        shutil.rmtree(temp_path, ignore_errors=True)
        if args.verbose:
            print("temp dir removed: "+temp_path+"\n")


if __name__ == "__main__":
    main()
